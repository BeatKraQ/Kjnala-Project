{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.kjnala.com\"\n",
    "search_url = f'{BASE_URL}/board/index.jsp?code=bbs018&page=1'\n",
    "page = requests.get(search_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "num = soup.select('td.pc_line')[0].text\n",
    "end_num = math.ceil(int(num) / 10)\n",
    "\n",
    "df = pd.read_csv('./통합.csv')\n",
    "stop_date = df['date'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(date, dtype, link):\n",
    "    search_url = f'{BASE_URL}{link}'\n",
    "    page = requests.get(search_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    items = soup.select('div.contArea > p')\n",
    "    \n",
    "    # Filter items with more than 9 characters\n",
    "    filtered_items = [item for item in items if len(item.text) >= 9]\n",
    "\n",
    "    # Reassign items with filtered items\n",
    "    items = filtered_items\n",
    "    \n",
    "    count = str(items).count('<p>')\n",
    "    \n",
    "    item_list = []\n",
    "    \n",
    "    for i in range(len(items)):\n",
    "        \n",
    "        if '<br/>' in str(items[i]):\n",
    "            item_t = re.sub('<p>', '', str(items[i]))\n",
    "            item_t = re.sub('</p>', '', item_t)\n",
    "            item_t = item_t.replace(u'\\xa0', u'')\n",
    "            item_t = item_t.split('<br/>')\n",
    "            while(\"\" in item_t):\n",
    "                item_t.remove(\"\")\n",
    "\n",
    "            item_t =[s[1:].lstrip() if s.startswith('-') else s.lstrip('- ').lstrip() for s in item_t]\n",
    "            \n",
    "            if count > 3:\n",
    "                item_t = [''.join(item_t)]\n",
    "            \n",
    "            for i in range(len(item_t)):\n",
    "                if '-' in item_t[i]:\n",
    "                    item = item_t[i].split('-', 1)[0]\n",
    "                    item2 = item_t[i].split('-', 1)[1]\n",
    "                    item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "                elif ' ' in item_t[i]:\n",
    "                    item = item_t[i].split(' ', 1)[0]\n",
    "                    item2 = item_t[i].split(' ', 1)[1]\n",
    "                    item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "                else: \n",
    "                    item_list.append([date, item_t[i], np.nan, dtype])\n",
    "\n",
    "    if item_list == []:\n",
    "        for i in range(len(items)):\n",
    "            item_t = re.sub('<p>', '', str(items[i]))\n",
    "            item_t = re.sub('</p>', '', item_t)\n",
    "            item_t = item_t.replace(u'\\xa0', u'')\n",
    "                    \n",
    "            if '-' in item_t:\n",
    "                item = item_t.split('-', 1)[0]\n",
    "                item2 = item_t.split('-', 1)[1]\n",
    "                item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "            elif ' ' in item_t:\n",
    "                item = item_t.split(' ', 1)[0]\n",
    "                item2 = item_t.split(' ', 1)[1]\n",
    "                item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "            else: \n",
    "                item_list.append([date, item_t[i], np.nan, dtype])\n",
    "    \n",
    "    return item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "def get_link(page):\n",
    "    search_url = f'{BASE_URL}/board/index.jsp?code=bbs018&page={page}'\n",
    "    page = requests.get(search_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    global stop_reached  # Use global flag\n",
    "    \n",
    "    nname = soup.select('p.subject > a')\n",
    "    \n",
    "    for i in range(len(nname)):\n",
    "        title = re.sub('\\s+', '', nname[i].text)\n",
    "        if '사위' in title:\n",
    "            dtype = '사위일체'\n",
    "        elif '효자' in title:\n",
    "            dtype = '효자종목'\n",
    "        elif '저점' in title:\n",
    "            dtype = '저점장대양봉'\n",
    "        else:\n",
    "            continue\n",
    "        number = re.findall(\"\\d+\", title)\n",
    "        date = '-'.join(number)[0:10]\n",
    "        \n",
    "        if date == stop_date:\n",
    "            stop_reached = True  # Set the flag to indicate stop_date is reached\n",
    "            return data  # Exit the function\n",
    "        \n",
    "        # Skip get_items if the length of the date string is less than 5\n",
    "        if len(date) < 5:\n",
    "            continue\n",
    "        \n",
    "        # 혹여나 코드로 인해 해당 페이지가 안보일 경우\n",
    "        #link = re.sub('code=bbs018&', '', nname[i]['href'])\n",
    "        \n",
    "        link = nname[i]['href']\n",
    "        try: \n",
    "            data.extend(get_items(date, dtype, link))\n",
    "            \n",
    "        except:\n",
    "            print(date)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_reached = False  # Flag to track stop_date condition\n",
    "for i in tqdm(range(1, end_num+1)): \n",
    "    data = get_link(i)\n",
    "    if stop_reached:  # Check if stop_date is reached inside get_link\n",
    "        break  # Break out of the loop if the stop_date is reached\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01 저점장대양봉 /board/read.jsp?id=1852426&code=bbs018&page=1\n",
      "2024-02-01 효자종목 /board/read.jsp?id=1852423&code=bbs018&page=1\n",
      "2024-02-01 사위일체 /board/read.jsp?id=1852422&code=bbs018&page=1\n",
      "2024-01-30 저점장대양봉 /board/read.jsp?id=1852041&code=bbs018&page=1\n"
     ]
    }
   ],
   "source": [
    "search_url = f'{BASE_URL}/board/index.jsp?code=bbs018&page={1}'\n",
    "page = requests.get(search_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "nname = soup.select('p.subject > a')\n",
    "\n",
    "for i in range(len(nname)):\n",
    "    title = re.sub('\\s+', '', nname[i].text)\n",
    "    if '사위' in title:\n",
    "        dtype = '사위일체'\n",
    "    elif '효자' in title:\n",
    "        dtype = '효자종목'\n",
    "    elif '저점' in title:\n",
    "        dtype = '저점장대양봉'\n",
    "    else:\n",
    "        continue\n",
    "    number = re.findall(\"\\d+\", title)\n",
    "    date = '-'.join(number)[0:10]\n",
    "    \n",
    "    if date == stop_date:\n",
    "        stop_reached = True  # Set the flag to indicate stop_date is reached\n",
    "        continue\n",
    "    \n",
    "    # Skip get_items if the length of the date string is less than 5\n",
    "    if len(date) < 5:\n",
    "        continue\n",
    "    \n",
    "    # 혹여나 코드로 인해 해당 페이지가 안보일 경우\n",
    "    #link = re.sub('code=bbs018&', '', nname[i]['href'])\n",
    "    \n",
    "    link = nname[i]['href']\n",
    "    print(date, dtype, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<script>alert('로그인이 필요한 서비스입니다.');</script><script>window.location.replace('/member/login.jsp?returl=/board/read.jsp?id=1852426&code=bbs018');</script>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_url = f'{BASE_URL}/board/read.jsp?id=1852426&code=bbs018'\n",
    "page = requests.get(search_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Login credentials\n",
    "username = 'stanfm'\n",
    "password = 'Bee4answer*'\n",
    "\n",
    "# Login endpoint\n",
    "login_url = 'https://www.kjnala.com/member/login.jsp'\n",
    "\n",
    "# Page to scrape after login\n",
    "scrape_url = 'https://www.kjnala.com/board/read.jsp?id=1852426&code=bbs018&page=1'\n",
    "\n",
    "# Create a session\n",
    "session = requests.Session()\n",
    "\n",
    "# Perform login\n",
    "login_data = {\n",
    "    'id': username,\n",
    "    'passwd': password\n",
    "}\n",
    "response = session.post(login_url, data=login_data)\n",
    "\n",
    "scrape_response = session.get(scrape_url)\n",
    "\n",
    "soup = BeautifulSoup(scrape_response.text, 'html.parser')\n",
    "soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2024-01-30', 'KG ETS', '도는 것 같다', '저점장대양봉'],\n",
       " ['2024-01-30', '동국씨엠', '좋은 움직임을 보여주는 거 같다', '저점장대양봉'],\n",
       " ['2024-01-30', '유진투자증권', '4000원 넘으면 다중봉 다중턱이 기다리고 있다', '저점장대양봉'],\n",
       " ['2024-01-30', '동양생명', '첩첩산중', '저점장대양봉'],\n",
       " ['2024-01-30', '피엠티', '아직 원매수를 털기엔 살짝 부족', '저점장대양봉'],\n",
       " ['2024-01-30', '삼성엔지니어링', '어제 떨어진거에 비해 음봉벽타기', '저점장대양봉'],\n",
       " ['2024-01-30', '다우데이타', '피하는게 좋다', '저점장대양봉'],\n",
       " ['2024-01-30', '쌍용C&amp;E', '월봉이 상대적으로 높다', '저점장대양봉'],\n",
       " ['2024-01-30', '메드팩토', '내리막폭포가 마음에 걸린다', '저점장대양봉'],\n",
       " ['2024-01-30', '오킨스전자', '다시 음봉벽타기를 하고 있다', '저점장대양봉'],\n",
       " ['2024-01-30', '미래에셋증권', '조금 더 가는 것 같다', '저점장대양봉'],\n",
       " ['2024-01-30', '삼성물산', '600일 신고가를 썼다', '저점장대양봉'],\n",
       " ['2024-01-30', '한국비엔씨', '슬슬 기지개를 피지 않을까', '저점장대양봉'],\n",
       " ['2024-01-30', '한국가스공사', '대형주들이 움직이니까 같이 움직이지 않나', '저점장대양봉'],\n",
       " ['2024-01-30', '휴맥스홀딩스', '반등하려고 애를 쓰고 있다', '저점장대양봉'],\n",
       " ['2024-01-30', '현대차', '워낙무겁다', '저점장대양봉'],\n",
       " ['2024-01-30', 'NH투자증권', '오늘 5프로 이상 상승하는데 이런 적이 거의 없는 무거운 종목이다', '저점장대양봉'],\n",
       " ['2024-01-30', 'DGB금융지주', '올라갈수록 첩첩산중이다 하도 물린 사람들이 많아서', '저점장대양봉'],\n",
       " ['2024-01-30', '디스플레이텍', '5000원수준까지 내려왔으니 관심을 가지고 봐야하는게 아닌지', '저점장대양봉'],\n",
       " ['2024-01-30', '한화', '좀 무서운 상황이다', '저점장대양봉'],\n",
       " ['2024-01-30', '미래에셋생명', '또 올라가면 다중봉 다중턱이 저 멀리서 기다리고 있다', '저점장대양봉'],\n",
       " ['2024-01-30', 'KB금융', '60000원 넘으면 다중봉 다중턱', '저점장대양봉'],\n",
       " ['2024-01-30', '아시아나IDT', '갭상승 장대음봉이어서 조심', '저점장대양봉'],\n",
       " ['2024-01-30', '한화투자증권', '힘들다', '저점장대양봉'],\n",
       " ['2024-01-30', 'BNK금융지주', '같이 올라가는 상황', '저점장대양봉'],\n",
       " ['2024-01-30', '하나금융지주', '마찬가지', '저점장대양봉'],\n",
       " ['2024-01-30', '기업은행', '마찬가지', '저점장대양봉'],\n",
       " ['2024-01-30', '에이스테크', '눌렸다가 반등', '저점장대양봉'],\n",
       " ['2024-01-30', '푸른저축은행', '조금 이제는 상승하지도 못하고 끝나지 않았나', '저점장대양봉'],\n",
       " ['2024-01-30',\n",
       "  '모아데이타',\n",
       "  '지지가 무너지면 잘못하면 1500원까지 갈 것 같은 생각이 드는 기분 나쁨',\n",
       "  '저점장대양봉'],\n",
       " ['2024-01-30', '우리바이오', '한두번은 기회를 주지 않을까', '저점장대양봉'],\n",
       " ['2024-01-30', '한화갤러리아트', '신규상장주 말고는 움직임이 좋다', '저점장대양봉'],\n",
       " ['2024-01-30', '엔케이맥스', '앞과 위가 흉측한 종목은 들어가지 마라', '저점장대양봉'],\n",
       " ['2024-01-30', '한성크린텍', '상한가가 무너졌으면 얼마나 좋을까 생각한다', '저점장대양봉'],\n",
       " ['2024-01-30', '흥국화재', '무너지면 괜찮지 않을까</br>', '저점장대양봉']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "search_url = f'{BASE_URL}{link}'\n",
    "scrape_response = session.get(search_url)\n",
    "soup = BeautifulSoup(scrape_response.text, 'html.parser')\n",
    "\n",
    "items = soup.select('div.contArea > p')\n",
    "\n",
    "# Filter items with more than 9 characters\n",
    "filtered_items = [item for item in items if len(item.text) >= 9]\n",
    "\n",
    "# Reassign items with filtered items\n",
    "items = filtered_items\n",
    "\n",
    "count = str(items).count('<p>')\n",
    "\n",
    "item_list = []\n",
    "\n",
    "for i in range(len(items)):\n",
    "    if '<br/>' in str(items[i]) or '<br>' in str(items[i]):\n",
    "        item_t = re.sub('<p>', '', str(items[i]))\n",
    "        item_t = re.sub('</p>', '', item_t)\n",
    "        item_t = item_t.replace(u'\\xa0', u'')\n",
    "        item_t = re.split(r'<br/>|<br>', item_t)\n",
    "        while(\"\" in item_t):\n",
    "            item_t.remove(\"\")\n",
    "\n",
    "        item_t =[s[1:].lstrip() if s.startswith('-') else s.lstrip('- ').lstrip() for s in item_t]\n",
    "        \n",
    "        if count > 3:\n",
    "            item_t = [''.join(item_t)]\n",
    "        \n",
    "        for i in range(len(item_t)):\n",
    "            if '-' in item_t[i]:\n",
    "                item = item_t[i].split('-', 1)[0]\n",
    "                item2 = item_t[i].split('-', 1)[1]\n",
    "                item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "            elif ' ' in item_t[i]:\n",
    "                item = item_t[i].split(' ', 1)[0]\n",
    "                item2 = item_t[i].split(' ', 1)[1]\n",
    "                item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "            else: \n",
    "                item_list.append([date, item_t[i], np.nan, dtype])\n",
    "\n",
    "if item_list == []:\n",
    "    for i in range(len(items)):\n",
    "        item_t = re.sub('<p>', '', str(items[i]))\n",
    "        item_t = re.sub('</p>', '', item_t)\n",
    "        item_t = item_t.replace(u'\\xa0', u'')\n",
    "                \n",
    "        if '-' in item_t:\n",
    "            item = item_t.split('-', 1)[0]\n",
    "            item2 = item_t.split('-', 1)[1]\n",
    "            item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "        elif ' ' in item_t:\n",
    "            item = item_t.split(' ', 1)[0]\n",
    "            item2 = item_t.split(' ', 1)[1]\n",
    "            item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "        else: \n",
    "            item_list.append([date, item_t[i], np.nan, dtype])\n",
    "    \n",
    "print(item_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('stock')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c48fe3414470e0e78456d36ccc4d279ba5f3d2668e7351207bf0b3dbee7a716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
