{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-15\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://www.kjnala.com\"\n",
    "search_url = f'{BASE_URL}/board/index.jsp?code=bbs018&page=1'\n",
    "page = requests.get(search_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "num = soup.select('td.pc_line')[0].text\n",
    "end_num = math.ceil(int(num) / 10)\n",
    "\n",
    "df = pd.read_csv('통합.csv', encoding = 'cp949')\n",
    "stop_date = df['date'].iloc[0]\n",
    "print(stop_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<script>parent.location.replace('/mypage/index.jsp');</script>\n"
     ]
    }
   ],
   "source": [
    "# Login credentials\n",
    "username = os.getenv('id1')\n",
    "password = os.getenv('pswd1')\n",
    "\n",
    "# Login endpoint\n",
    "login_url = 'https://www.kjnala.com/member/login.jsp'\n",
    "\n",
    "# Page to scrape after login\n",
    "scrape_url = 'https://www.kjnala.com/board/read.jsp?id=1852426&code=bbs018&page=1'\n",
    "\n",
    "# Create a session\n",
    "session = requests.Session()\n",
    "\n",
    "# Perform login\n",
    "login_data = {\n",
    "    'id': username,\n",
    "    'passwd': password\n",
    "}\n",
    "response = session.post(login_url, data=login_data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(date, dtype, link):\n",
    "    search_url = f'{BASE_URL}{link}'\n",
    "    scrape_response = session.get(search_url)\n",
    "    soup = BeautifulSoup(scrape_response.text, 'html.parser')\n",
    "\n",
    "    items = soup.select('div.contArea > p')\n",
    "    \n",
    "    # Filter items with more than 9 characters\n",
    "    filtered_items = [item for item in items if len(item.text) >= 9]\n",
    "\n",
    "    # Reassign items with filtered items\n",
    "    items = filtered_items\n",
    "    \n",
    "    count = str(items).count('<p>')\n",
    "    \n",
    "    item_list = []\n",
    "    \n",
    "    for i in range(len(items)):\n",
    "        if '<br/>' in str(items[i]) or '<br>' in str(items[i]):\n",
    "            item_t = re.sub('<p>', '', str(items[i]))\n",
    "            item_t = re.sub('</p>', '', item_t)\n",
    "            item_t = item_t.replace(u'\\xa0', u'')\n",
    "            item_t = re.split(r'<br/>|<br>', item_t)\n",
    "            while(\"\" in item_t):\n",
    "                item_t.remove(\"\")\n",
    "\n",
    "            item_t =[s[1:].lstrip() if s.startswith('-') else s.lstrip('- ').lstrip() for s in item_t]\n",
    "            \n",
    "            if count > 3:\n",
    "                item_t = [''.join(item_t)]\n",
    "            \n",
    "            for i in range(len(item_t)):\n",
    "                if '-' in item_t[i]:\n",
    "                    item = item_t[i].split('-', 1)[0]\n",
    "                    item2 = item_t[i].split('-', 1)[1]\n",
    "                    item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "                elif ' ' in item_t[i]:\n",
    "                    item = item_t[i].split(' ', 1)[0]\n",
    "                    item2 = item_t[i].split(' ', 1)[1]\n",
    "                    item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "                else: \n",
    "                    item_list.append([date, item_t[i], np.nan, dtype])\n",
    "\n",
    "    if item_list == []:\n",
    "        for i in range(len(items)):\n",
    "            item_t = re.sub('<p>', '', str(items[i]))\n",
    "            item_t = re.sub('</p>', '', item_t)\n",
    "            item_t = item_t.replace(u'\\xa0', u'')\n",
    "                    \n",
    "            if '-' in item_t:\n",
    "                item = item_t.split('-', 1)[0]\n",
    "                item2 = item_t.split('-', 1)[1]\n",
    "                item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "            elif ' ' in item_t:\n",
    "                item = item_t.split(' ', 1)[0]\n",
    "                item2 = item_t.split(' ', 1)[1]\n",
    "                item_list.append([date, item.rstrip(), item2.lstrip(), dtype])\n",
    "            else: \n",
    "                item_list.append([date, item_t[i], np.nan, dtype])\n",
    "    \n",
    "    return item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "def get_link(page):\n",
    "    search_url = f'{BASE_URL}/board/index.jsp?code=bbs018&page={page}'\n",
    "    page = requests.get(search_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    global stop_reached  # Use global flag\n",
    "    \n",
    "    nname = soup.select('p.subject > a')\n",
    "    \n",
    "    for i in range(len(nname)):\n",
    "        title = re.sub('\\s+', '', nname[i].text)\n",
    "        if '사위' in title:\n",
    "            dtype = '사위일체'\n",
    "        elif '효자' in title:\n",
    "            dtype = '효자종목'\n",
    "        elif '저점' in title:\n",
    "            dtype = '저점장대양봉'\n",
    "        else:\n",
    "            continue\n",
    "        number = re.findall(\"\\d+\", title)\n",
    "        date = '-'.join(number)[0:10]\n",
    "        \n",
    "        if date == stop_date:\n",
    "            stop_reached = True  # Set the flag to indicate stop_date is reached\n",
    "            return data  # Exit the function\n",
    "        \n",
    "        # Skip get_items if the length of the date string is less than 5\n",
    "        if len(date) < 5:\n",
    "            continue\n",
    "        \n",
    "        link = nname[i]['href']\n",
    "        \n",
    "        try: \n",
    "            data.extend(get_items(date, dtype, link))\n",
    "            \n",
    "        except:\n",
    "            print(date)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/70 [00:47<08:31,  7.99s/it]\n"
     ]
    }
   ],
   "source": [
    "stop_reached = False  # Flag to track stop_date condition\n",
    "for i in tqdm(range(1, end_num+1)): \n",
    "    data = get_link(i)\n",
    "    if stop_reached:  # Check if stop_date is reached inside get_link\n",
    "        break  # Break out of the loop if the stop_date is reached\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['date', 'item', 'exp', 'type'])\n",
    "df['integrated_category'] = '' \n",
    "df['item'] = df['item'].str.replace('amp;', '')\n",
    "df['exp'] = df['exp'].str.replace('exp;', '')\n",
    "df['exp'] =df['exp'].str.replace('</br>', '')\n",
    "df = df[~df.item.str.contains('=', na=False)]\n",
    "df = df[~df.exp.str.contains('=', na=False)]\n",
    "\n",
    "length_condition = df['item'].apply(lambda x: len(str(x))) >= 2\n",
    "df = df[length_condition]\n",
    "\n",
    "# 설명 없는 종목 제거\n",
    "df = df.dropna(subset=['exp'])\n",
    "\n",
    "# '금일 사위일체는 없습니다' 제거\n",
    "condition = df['item'].str.contains('사위일체') | df['exp'].str.contains('사위일체')\n",
    "df = df[~condition]\n",
    "\n",
    "# 종목 이름에 '효자'있는 종목 제거\n",
    "df = df[~df['item'].str.contains('효자')]\n",
    "\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_word = ['잘못', '큰일', '버리자', '오케이', '애매', '멀리', '피한', '리스크', '걸린다', '피하', '본척', '피해', '아슬', '우려', '못쓰고', '더 좋은', '다음', '즐비', '위험', \n",
    "           '말함', '앉음', '꺼려', '비리비리', '내리막', '무섭', '무조건', '안된다', '뒷설거지', '거둬야', '많은분', '어렵', '스탑', '두렵', '다소', '경계', '허봉', '멀다', \n",
    "           '흘러', '무너질지', '부담', '갈수록', '피해야', '비교', '종목에 대한 사랑', '톱니바퀴', '피함', '못넘고', '벽타기', '못먹어도', '불안', '아님', '목장', '기다려야', \n",
    "           '밀리', '털었다', '흉측', '버려', '붕괴', '결투']\n",
    "df = df[~df.exp.str.contains('|'.join(nt_word))]\n",
    "df = df.reset_index(drop=True)\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>item</th>\n",
       "      <th>exp</th>\n",
       "      <th>type</th>\n",
       "      <th>integrated_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-09</td>\n",
       "      <td>스마트레이더시스템</td>\n",
       "      <td>좀 더 지켜봐야한다</td>\n",
       "      <td>저점장대양봉</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-09</td>\n",
       "      <td>HD현대에너지솔루션</td>\n",
       "      <td>월봉을 보면 좀 더 내려오는게 안전하지 않나</td>\n",
       "      <td>저점장대양봉</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-09</td>\n",
       "      <td>나이벡</td>\n",
       "      <td>이만원 넘으면 다중봉 다중턱</td>\n",
       "      <td>저점장대양봉</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-09</td>\n",
       "      <td>한화투자증권</td>\n",
       "      <td>앞폭탄 맞고 떨어지는 상황</td>\n",
       "      <td>저점장대양봉</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-09</td>\n",
       "      <td>세기상사</td>\n",
       "      <td>상황을 지켜보는 것도 하나의 방법</td>\n",
       "      <td>저점장대양봉</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>동구바이오제약</td>\n",
       "      <td>앞으로 조금 더 가지 않을까</td>\n",
       "      <td>효자종목</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>파미셀</td>\n",
       "      <td>이 종목도 더욱더 우리한테 기회를 줄 것 같다</td>\n",
       "      <td>효자종목</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>솔트웨어</td>\n",
       "      <td>반갑기 그지 없다</td>\n",
       "      <td>효자종목</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>디와이디</td>\n",
       "      <td>지금 사면 더 싸게 사는 상황이 아닌가</td>\n",
       "      <td>사위일체</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>제주맥주</td>\n",
       "      <td>앞으로도 팍팍 가서 2천원 3천원 갔으면 좋겠다</td>\n",
       "      <td>사위일체</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>636 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date        item                          exp    type  \\\n",
       "0    2024-04-09   스마트레이더시스템                   좀 더 지켜봐야한다  저점장대양봉   \n",
       "1    2024-04-09  HD현대에너지솔루션     월봉을 보면 좀 더 내려오는게 안전하지 않나  저점장대양봉   \n",
       "2    2024-04-09         나이벡              이만원 넘으면 다중봉 다중턱  저점장대양봉   \n",
       "3    2024-04-09      한화투자증권               앞폭탄 맞고 떨어지는 상황  저점장대양봉   \n",
       "4    2024-04-09        세기상사           상황을 지켜보는 것도 하나의 방법  저점장대양봉   \n",
       "..          ...         ...                          ...     ...   \n",
       "631  2024-03-18     동구바이오제약              앞으로 조금 더 가지 않을까    효자종목   \n",
       "632  2024-03-18         파미셀    이 종목도 더욱더 우리한테 기회를 줄 것 같다    효자종목   \n",
       "633  2024-03-18        솔트웨어                    반갑기 그지 없다    효자종목   \n",
       "634  2024-03-18        디와이디        지금 사면 더 싸게 사는 상황이 아닌가    사위일체   \n",
       "635  2024-03-18        제주맥주  앞으로도 팍팍 가서 2천원 3천원 갔으면 좋겠다     사위일체   \n",
       "\n",
       "    integrated_category  \n",
       "0                        \n",
       "1                        \n",
       "2                        \n",
       "3                        \n",
       "4                        \n",
       "..                  ...  \n",
       "631                      \n",
       "632                      \n",
       "633                      \n",
       "634                      \n",
       "635                      \n",
       "\n",
       "[636 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 3s 39ms/step - loss: 3.7412 - accuracy: 0.1792 - val_loss: 3.4301 - val_accuracy: 0.1992\n",
      "Epoch 2/100\n",
      " 7/20 [=========>....................] - ETA: 0s - loss: 3.4632 - accuracy: 0.1797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\stock\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 25ms/step - loss: 3.4154 - accuracy: 0.1782 - val_loss: 3.2805 - val_accuracy: 0.1992\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 3.2929 - accuracy: 0.1992 - val_loss: 3.1448 - val_accuracy: 0.1992\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 3.0176 - accuracy: 0.2308 - val_loss: 2.8337 - val_accuracy: 0.2742\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2.6535 - accuracy: 0.2988 - val_loss: 2.5770 - val_accuracy: 0.3427\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 2.3412 - accuracy: 0.3836 - val_loss: 2.3732 - val_accuracy: 0.4008\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2.0860 - accuracy: 0.4421 - val_loss: 2.2450 - val_accuracy: 0.4379\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 1.8455 - accuracy: 0.4978 - val_loss: 2.1177 - val_accuracy: 0.4944\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1.6376 - accuracy: 0.5579 - val_loss: 2.0733 - val_accuracy: 0.5194\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1.4395 - accuracy: 0.6009 - val_loss: 2.0121 - val_accuracy: 0.5589\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1.2695 - accuracy: 0.6473 - val_loss: 2.0458 - val_accuracy: 0.5790\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1.1190 - accuracy: 0.6816 - val_loss: 1.9951 - val_accuracy: 0.5919\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 0.9951 - accuracy: 0.7201 - val_loss: 2.0072 - val_accuracy: 0.6048\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.8720 - accuracy: 0.7482 - val_loss: 2.0270 - val_accuracy: 0.6089\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.7763 - accuracy: 0.7732 - val_loss: 2.0686 - val_accuracy: 0.6282\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.6947 - accuracy: 0.8021 - val_loss: 2.1865 - val_accuracy: 0.6250\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 0.6516 - accuracy: 0.8103 - val_loss: 2.2069 - val_accuracy: 0.6298\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 0.5772 - accuracy: 0.8267 - val_loss: 2.1835 - val_accuracy: 0.6540\n",
      "39/39 [==============================] - 0s 4ms/step\n",
      "Accuracy for CNN: 0.5919354838709677\n",
      "Classification Report for CNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       247\n",
      "           1       0.48      0.71      0.57        41\n",
      "           2       0.39      0.64      0.49        14\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.59      0.81      0.69        43\n",
      "           6       0.50      0.50      0.50         4\n",
      "           7       0.41      0.57      0.48        28\n",
      "           8       0.27      0.58      0.37        48\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.15      0.10      0.12        21\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.81      0.76      0.79        17\n",
      "          13       0.28      0.52      0.36        31\n",
      "          14       0.00      0.00      0.00         5\n",
      "          15       0.27      0.57      0.36        14\n",
      "          16       0.50      0.55      0.52        11\n",
      "          17       0.52      0.85      0.65        26\n",
      "          18       0.33      0.33      0.33        33\n",
      "          19       0.00      0.00      0.00        15\n",
      "          20       0.60      0.68      0.64        31\n",
      "          21       0.00      0.00      0.00        15\n",
      "          22       0.00      0.00      0.00         6\n",
      "          23       0.17      0.14      0.16        28\n",
      "          24       0.58      0.64      0.61        33\n",
      "          25       0.00      0.00      0.00         3\n",
      "          26       0.53      0.38      0.44        21\n",
      "          27       0.76      0.63      0.69       105\n",
      "          28       0.70      0.94      0.80        17\n",
      "          29       0.00      0.00      0.00         1\n",
      "          30       0.00      0.00      0.00         8\n",
      "          31       0.00      0.00      0.00         8\n",
      "          32       0.00      0.00      0.00         8\n",
      "          33       0.00      0.00      0.00         2\n",
      "          34       0.00      0.00      0.00         8\n",
      "          35       0.42      0.47      0.45        36\n",
      "          36       0.00      0.00      0.00         8\n",
      "          37       0.00      0.00      0.00         9\n",
      "          38       0.00      0.00      0.00         4\n",
      "          39       0.96      0.93      0.94        99\n",
      "          40       0.00      0.00      0.00         6\n",
      "          41       0.77      0.89      0.83        19\n",
      "          42       0.00      0.00      0.00        14\n",
      "          43       0.00      0.00      0.00         2\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.60      0.75      0.67         8\n",
      "          46       0.81      0.89      0.85        19\n",
      "          47       0.00      0.00      0.00         8\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.00      0.00      0.00         8\n",
      "          50       0.41      0.48      0.44        25\n",
      "          51       0.89      0.90      0.89        60\n",
      "\n",
      "    accuracy                           0.59      1240\n",
      "   macro avg       0.28      0.33      0.30      1240\n",
      "weighted avg       0.55      0.59      0.56      1240\n",
      "\n",
      "20/20 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\stock\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\stock\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\stock\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set seeds to ensure reproducibility\n",
    "tf.random.set_seed(2)  # TensorFlow random seed\n",
    "np.random.seed(2)     # Numpy random seed\n",
    "random.seed(2)        # Python random seed\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('통합.csv', encoding = 'cp949')# Replace with your data file\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = data['exp']\n",
    "y = data['integrated_category']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "max_words = len(tokenizer.index_word) + 1\n",
    "# Pad sequences to ensure uniform length\n",
    "max_seq_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_val_seq)) # Change as needed based on your data\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_seq_length)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "\n",
    "# Build CNN model\n",
    "\"\"\" model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(max_words, 32, input_length=max_seq_length))\n",
    "model_cnn.add(Conv1D(256, 10, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(144, activation='relu'))\n",
    "model_cnn.add(Dropout(0.4))\n",
    "model_cnn.add(Dense(96, activation='relu'))\n",
    "model_cnn.add(Dropout(0.2)) \"\"\"\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(max_words, 32, input_length=max_seq_length))\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dropout(0.4))\n",
    "model_cnn.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile and train the CNN model\n",
    "model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the CNN model with callbacks\n",
    "model_cnn.fit(X_train_pad, y_train, epochs=100, batch_size=256, validation_data = (X_val_pad, y_val), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Evaluate the CNN model\n",
    "y_pred_proba_cnn = model_cnn.predict(X_val_pad)  # Get predicted probabilities for each class\n",
    "y_pred_cnn = y_pred_proba_cnn.argmax(axis=1)  # Extract classes based on highest probability\n",
    "\n",
    "accuracy_cnn = accuracy_score(y_val, y_pred_cnn)\n",
    "classification_rep_cnn = classification_report(y_val, y_pred_cnn)\n",
    "\n",
    "print(f\"Accuracy for CNN: {accuracy_cnn}\")\n",
    "print(f\"Classification Report for CNN:\\n{classification_rep_cnn}\")\n",
    "\n",
    "# Assuming df2 has a column 'exp' containing text data\n",
    "\n",
    "# Tokenize text data in df2\n",
    "X_new = df['exp']\n",
    "X_new_seq = tokenizer.texts_to_sequences(X_new)\n",
    "X_new_pad = pad_sequences(X_new_seq, maxlen=max_seq_length)\n",
    "\n",
    "# Predict category values using the trained CNN model\n",
    "y_pred_proba_new = model_cnn.predict(X_new_pad)\n",
    "y_pred_new = y_pred_proba_new.argmax(axis=1)\n",
    "\n",
    "# Convert predicted labels back to category names (if needed)\n",
    "predicted_categories = label_encoder.inverse_transform(y_pred_new)\n",
    "\n",
    "# Add predicted values to a new column in df2\n",
    "df['integrated_category'] = predicted_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy: 0.7482517482517482\n",
      "F1 Score: 0.7210677869082457\n"
     ]
    }
   ],
   "source": [
    "test_answer = pd.read_csv('test_answer.csv', encoding = 'cp949')\n",
    "X_test = test_answer['exp']\n",
    "y_test = test_answer['integrated_category']\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length)\n",
    "\n",
    "pred = model_cnn.predict(X_test_pad)\n",
    "pred_new = pred.argmax(axis=1)\n",
    "\n",
    "predicted_categories2 = label_encoder.inverse_transform(pred_new)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_categories2)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, predicted_categories2, average='weighted')  # You can choose 'macro' or 'micro' if needed\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([df, data], ignore_index=True)\n",
    "concatenated_df.to_csv('통합.csv', index=False, encoding='cp949')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('stock')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c48fe3414470e0e78456d36ccc4d279ba5f3d2668e7351207bf0b3dbee7a716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
